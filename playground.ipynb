{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import joblib\n",
    "import torch\n",
    "import trimesh\n",
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import cv2\n",
    "from PIL import Image, ImageOps\n",
    "from tqdm.notebook import tqdm\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "\n",
    "sys.path.append('/home/sid/Projects/reprojection')\n",
    "sys.path.append('/home/sid/Projects/humor/humor')\n",
    "\n",
    "from body_model.body_model import BodyModel\n",
    "from humor_inference.reproject_humor_sequence import get_camera_params, make_44\n",
    "from reproject.reproject_mesh.run_reprojection import (\n",
    "    render_mesh, \n",
    "    get_meshes,\n",
    "    get_synced_meshes,\n",
    "    render_mesh,\n",
    "    transform_meshes \n",
    ")\n",
    "from fitting.fitting_utils import run_smpl\n",
    "from humor_inference.reproject_wham_sequence import sanitize_wham_preds, get_wham_mesh_sequence, get_wham_parameters, transform_mesh_sequence\n",
    "from reproject.reproject_mesh.run_reprojection_wham import MeshSynchronizer, MeshProcessor, Renderer\n",
    "from reproject.reproject_mesh.reprojection_utils import get_calib_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_coordinate_system(mesh, transform_option='identity', save_path=None):\n",
    "    \"\"\"\n",
    "    Visualize mesh with different coordinate transformations.\n",
    "    \n",
    "    Args:\n",
    "        mesh (trimesh.Trimesh): Input mesh to visualize\n",
    "        transform_option (str): Transformation to apply:\n",
    "            - 'identity': No transformation\n",
    "            - 'invert_yz': Invert Y and Z axes\n",
    "            - 'invert_xz': Invert X and Z axes\n",
    "            - 'rotate_y_180': Rotate 180Â° around Y\n",
    "            - 'swap_yz': Swap Y and Z axes\n",
    "            - 'invert_z': Invert Z-axis\n",
    "        save_path (str, optional): Path to save visualization\n",
    "    \"\"\"\n",
    "    # Create transformation matrix\n",
    "    if transform_option == 'identity':\n",
    "        alignment_matrix = np.eye(4)\n",
    "    elif transform_option == 'invert_yz':\n",
    "        alignment_matrix = np.eye(4)\n",
    "        alignment_matrix[1, 1] = -1\n",
    "        alignment_matrix[2, 2] = -1\n",
    "    elif transform_option == 'invert_xz':\n",
    "        alignment_matrix = np.eye(4)\n",
    "        alignment_matrix[0, 0] = -1\n",
    "        alignment_matrix[2, 2] = -1\n",
    "    elif transform_option == 'rotate_y_180':\n",
    "        alignment_matrix = trimesh.transformations.rotation_matrix(np.pi, [0, 1, 0])\n",
    "    elif transform_option == 'swap_yz':\n",
    "        alignment_matrix = np.array([\n",
    "            [1, 0, 0, 0], \n",
    "            [0, 0, 1, 0],\n",
    "            [0, 1, 0, 0],\n",
    "            [0, 0, 0, 1],\n",
    "        ])\n",
    "    elif transform_option == 'invert_z':\n",
    "        alignment_matrix = np.eye(4)\n",
    "        alignment_matrix[2, 2] = -1\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown option '{transform_option}'\")\n",
    "\n",
    "    # Apply transformation\n",
    "    mesh = mesh.copy()\n",
    "    mesh.apply_transform(alignment_matrix)\n",
    "\n",
    "    # Visualization\n",
    "    fig = plt.figure(figsize=(15, 5))\n",
    "    views = [(0, 0), (90, 0), (0, 90)]\n",
    "    titles = ['Front (X-Y)', 'Side (Y-Z)', 'Top (X-Z)']\n",
    "    \n",
    "    for i, (elev, azim) in enumerate(views, 1):\n",
    "        ax = fig.add_subplot(1, 3, i, projection='3d')\n",
    "        \n",
    "        # Plot vertices\n",
    "        vertices = mesh.vertices\n",
    "        ax.scatter(vertices[:, 0], vertices[:, 1], vertices[:, 2], \n",
    "                  c='b', marker='.', s=1, alpha=0.1)\n",
    "        \n",
    "        # Plot axes\n",
    "        origin = mesh.centroid\n",
    "        axis_length = np.max(np.ptp(vertices, axis=0)) * 0.2\n",
    "        \n",
    "        ax.quiver(origin[0], origin[1], origin[2], \n",
    "                 axis_length, 0, 0, color='r', label='X')\n",
    "        ax.quiver(origin[0], origin[1], origin[2], \n",
    "                 0, axis_length, 0, color='g', label='Y')\n",
    "        ax.quiver(origin[0], origin[1], origin[2], \n",
    "                 0, 0, axis_length, color='b', label='Z')\n",
    "        \n",
    "        ax.set_title(f\"{titles[i-1]}\\n{transform_option}\")\n",
    "        ax.view_init(elev=elev, azim=azim)\n",
    "        ax.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path)\n",
    "        plt.close()\n",
    "    else:\n",
    "        plt.show()\n",
    "    \n",
    "    # Print mesh statistics\n",
    "    print(f\"\\nMesh Statistics ({transform_option}):\")\n",
    "    print(f\"Centroid: {mesh.centroid}\")\n",
    "    print(f\"Bounds: {mesh.bounds}\")\n",
    "    print(f\"Up vector (Z) mean: {mesh.vertices[:, 2].mean():.3f}\")\n",
    "    return mesh\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HUMOR_OUT_FILE = \"/home/NAS-mountpoint/kinect-omni-ego/2023-02-09/at-unis/lab/a04-2/capture0/out_capture0/results_out/final_results/stage3_results.npz\"\n",
    "WHAM_OUT_FILE = '/home/sid/Projects/WHAM/output/demo/2023-02-09_at-unis_lab_a01_capture0/wham_output.pkl'\n",
    "BM_PATH = \"/home/sid/Projects/humor/body_models/smplh/male/model.npz\"\n",
    "\n",
    "humor_output = np.load(HUMOR_OUT_FILE)\n",
    "wham_output = joblib.load(WHAM_OUT_FILE)\n",
    "\n",
    "# Setup device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Initialize BodyModel with 22 betas\n",
    "bm = BodyModel(\n",
    "    bm_path=BM_PATH,\n",
    "    num_betas=22,  # Changed to match SMPLH shape space\n",
    "    batch_size=1,\n",
    "    model_type='smplh'\n",
    ").to(device)\n",
    "\n",
    "# Load WHAM data and pad betas to 22\n",
    "wham_data = wham_output['results']\n",
    "wham_betas = torch.tensor(wham_data[0][\"betas\"][0], device=device).float()  # [10]\n",
    "wham_betas = torch.cat([\n",
    "    wham_betas,\n",
    "    torch.zeros(12, device=device)  # Pad to 22 dims\n",
    "])\n",
    "\n",
    "# Rest of WHAM processing\n",
    "wham_pose = torch.tensor(wham_data[0][\"pose_world\"][0], device=device).float()\n",
    "wham_trans = torch.tensor(wham_data[0][\"trans_world\"][0], device=device).float()\n",
    "\n",
    "# Split pose correctly for SMPLH\n",
    "root_orient = wham_pose[:3].reshape(1, -1)     # [1, 3]\n",
    "pose_body = wham_pose[3:66].reshape(1, -1)     # [1, 63]\n",
    "hand_pose = torch.zeros(1, 90, device=device)  # [1, 90]\n",
    "\n",
    "# Pass to BodyModel with padded betas\n",
    "try:\n",
    "    wham_body = bm(\n",
    "        pose_body=pose_body,        \n",
    "        root_orient=root_orient,    \n",
    "        pose_hand=hand_pose,        \n",
    "        trans=trans,                \n",
    "        betas=wham_betas.view(1, -1)  # Now [1, 16]\n",
    "    )\n",
    "except RuntimeError as e:\n",
    "    print(\"Error during BodyModel forward pass:\", e)\n",
    "    raise\n",
    "\n",
    "# Create Trimesh object for WHAM mesh\n",
    "wham_mesh = trimesh.Trimesh(\n",
    "    vertices=wham_body.v.detach().cpu().numpy()[0],\n",
    "    faces=bm.bm.faces_tensor.cpu().numpy()  # Changed from bm.f to bm.bm.faces_tensor\n",
    ")\n",
    "\n",
    "# Load and process HuMoR mesh\n",
    "# Load HuMoR results\n",
    "humor_res = np.load(HUMOR_OUT_FILE)\n",
    "\n",
    "# Convert HuMoR parameters to tensors and pad betas to 16 dimensions\n",
    "humor_betas = torch.tensor(humor_res[\"betas\"][0], device=device).float()      # [10]\n",
    "humor_betas = torch.cat([                                                     # Pad to [16]\n",
    "    humor_betas,\n",
    "    torch.zeros(6, device=device)\n",
    "])\n",
    "humor_trans = torch.tensor(humor_res[\"trans\"][0], device=device).float()      # [3]\n",
    "humor_pose = torch.tensor(humor_res[\"pose_body\"][0], device=device).float()   # [63]\n",
    "humor_root = torch.tensor(humor_res[\"root_orient\"][0], device=device).float() # [3]\n",
    "\n",
    "# Initialize zero hand pose\n",
    "humor_hand_pose = torch.zeros(1, 90, device=device)  # [1, 90] for SMPLH hands\n",
    "\n",
    "# Get SMPL output for HuMoR parameters with padded betas\n",
    "humor_body = bm(\n",
    "    pose_body=humor_pose.view(1, -1),     # [1, 63]\n",
    "    root_orient=humor_root.view(1, -1),    # [1, 3] \n",
    "    pose_hand=humor_hand_pose,            # [1, 90]\n",
    "    trans=humor_trans.view(1, -1),        # [1, 3]\n",
    "    betas=humor_betas.view(1, -1)         # [1, 16] - now padded\n",
    ")\n",
    "# Create trimesh from HuMoR SMPL output\n",
    "humor_mesh = trimesh.Trimesh(\n",
    "    vertices=humor_body.v.detach().cpu().numpy()[0],\n",
    "    faces=bm.bm.faces_tensor.cpu().numpy()\n",
    ")\n",
    "\n",
    "# Compare coordinate systems\n",
    "print(\"WHAM Original:\")\n",
    "wham_orig = visualize_coordinate_system(wham_mesh, 'identity')\n",
    "\n",
    "print(\"\\nHuMoR Original:\")\n",
    "humor_orig = visualize_coordinate_system(humor_mesh, 'identity')\n",
    "\n",
    "# Apply transformations\n",
    "transforms = ['invert_z', 'invert_yz', 'swap_yz']\n",
    "for transform in transforms:\n",
    "    print(f\"\\nTrying {transform}:\")\n",
    "    visualize_coordinate_system(wham_mesh, transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_mesh_data(\n",
    "    mesh_type: str,  # Just \"wham\" or \"humor\"\n",
    "    input_file: str,\n",
    "    bm: 'BodyModel', \n",
    "    device: torch.device\n",
    ") -> trimesh.Trimesh:\n",
    "    \"\"\"Process mesh data for either WHAM or HuMoR outputs.\"\"\"\n",
    "    if mesh_type.lower() not in [\"wham\", \"humor\"]:\n",
    "        raise ValueError(\"mesh_type must be either 'wham' or 'humor'\")\n",
    "        \n",
    "    # Rest of function remains same but with string comparison\n",
    "    if mesh_type.lower() == \"wham\":\n",
    "        data = joblib.load(input_file)\n",
    "        betas = torch.tensor(data[0][\"betas\"][0], device=device).float()\n",
    "        pose = torch.tensor(data[0][\"pose_world\"][0], device=device).float()\n",
    "        trans = torch.tensor(data[0][\"trans_world\"][0], device=device).float()\n",
    "        beta_padding = 12  # Pad from 10 to 22\n",
    "    elif mesh_type.lower() == \"humor\":\n",
    "        data = np.load(input_file)\n",
    "        betas = torch.tensor(data[\"betas\"][0], device=device).float()\n",
    "        pose = torch.tensor(data[\"pose_body\"][0], device=device).float()\n",
    "        trans = torch.tensor(data[\"trans\"][0], device=device).float()\n",
    "        root_orient = torch.tensor(data[\"root_orient\"][0], device=device).float()\n",
    "        beta_padding = 6  # Pad from 16 to 22\n",
    "    else:\n",
    "        raise ValueError(\"Unknown mesh type\")\n",
    "\n",
    "    # Common processing\n",
    "    betas = torch.cat([betas, torch.zeros(beta_padding, device=device)])\n",
    "    hand_pose = torch.zeros(1, 90, device=device)\n",
    "    \n",
    "    # Handle poses differently based on type\n",
    "    if mesh_type == \"wham\":\n",
    "        root_orient = pose[:3].reshape(1, -1)\n",
    "        pose_body = pose[3:66].reshape(1, -1)\n",
    "    else:\n",
    "        root_orient = root_orient.view(1, -1)\n",
    "        pose_body = pose.view(1, -1)\n",
    "    \n",
    "    # Get body output\n",
    "    try:\n",
    "        body = bm(\n",
    "            pose_body=pose_body,\n",
    "            root_orient=root_orient,\n",
    "            pose_hand=hand_pose,\n",
    "            trans=trans.view(1, -1),\n",
    "            betas=betas.view(1, -1)\n",
    "        )\n",
    "    except RuntimeError as e:\n",
    "        print(f\"Error processing {mesh_type} mesh:\", e)\n",
    "        raise\n",
    "        \n",
    "    # Create and return mesh\n",
    "    return trimesh.Trimesh(\n",
    "        vertices=body.v.detach().cpu().numpy()[0],\n",
    "        faces=bm.bm.faces_tensor.cpu().numpy()\n",
    "    )\n",
    "\n",
    "# Example usage\n",
    "# FOLDER_CHOSEN = \"/home/NAS-mountpoint/kinect-omni-ego/2023-02-09/at-unis/lab/a04-2/capture0/\"\n",
    "# HUMOR_MESH = os.path.join(FOLDER_CHOSEN, \"out_capture0/results_out/final_results/stage3_results.npz\")\n",
    "# BM_PATH = \"/home/sid/Projects/humor/body_models/smplh/male/model.npz\"\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# # Initialize BodyModel with 22 betas\n",
    "# bm = BodyModel(\n",
    "#     bm_path=BM_PATH,\n",
    "#     num_betas=22,  # Changed to match SMPLH shape space\n",
    "#     batch_size=1,\n",
    "#     model_type='smplh'\n",
    "# ).to(device)\n",
    "# humor_mesh = process_mesh_data(\"humor\", HUMOR_MESH, bm, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Script that uses the saved reprojected HuMoR meshes directly to render them on the omni images. \n",
    "\n",
    "ROOT_DIR = \"/home/NAS-mountpoint/kinect-omni-ego/2024-01-12/at-unis/lab/sid/capture0\"\n",
    "\n",
    "LATERAL_VIEW_IMAGES = os.path.join(ROOT_DIR, \"rgb\")\n",
    "OMNI_IMAGES = os.path.join(os.path.dirname(ROOT_DIR), \"omni\")\n",
    "print(f\"Root directory: {ROOT_DIR}\")\n",
    "print(f\"Lateral view images: {LATERAL_VIEW_IMAGES}\")\n",
    "print(f\"Omni images: {OMNI_IMAGES}\")\n",
    "\n",
    "MESH_DIR = os.path.join(ROOT_DIR, \"out_capture0\", \"results_out\", \"final_results\")\n",
    "HUMOR_MESH_FILE = os.path.join(MESH_DIR, \"stage3_results.npz\")\n",
    "REPROJECTED_MESHES_FOLDER = os.path.join(ROOT_DIR, \"out_capture0\", \"reprojected\", \"meshes\")\n",
    "\n",
    "# Render the reprojected meshes onto omni images using intrinsics.\n",
    "OMNI_CALIB_FILE = \"/home/sid/Projects/reprojection/calibration/intrinsics/omni_calib.pkl\"\n",
    "with open(OMNI_CALIB_FILE, \"rb\") as f:\n",
    "    cam1_calib = pickle.load(f)\n",
    "\n",
    "images = sorted(os.listdir(OMNI_IMAGES))\n",
    "mesh_files = sorted(os.listdir(REPROJECTED_MESHES_FOLDER))\n",
    "assert len(images) == len(mesh_files), \"Number of images and meshes do not match\"\n",
    "\n",
    "def view_image(index):\n",
    "    mesh_file = mesh_files[index]\n",
    "    img_path = images[index]\n",
    "    \n",
    "    mesh = trimesh.load(os.path.join(REPROJECTED_MESHES_FOLDER, mesh_file))\n",
    "    use_omni, camera_matrix, xi, dist_coeffs = get_camera_params(cam1_calib)\n",
    "    \n",
    "    if use_omni:\n",
    "        vertices_2d, _ = cv2.omnidir.projectPoints(\n",
    "            np.expand_dims(mesh.vertices, axis=0),\n",
    "            np.zeros(3),\n",
    "            np.zeros(3),\n",
    "            camera_matrix,\n",
    "            xi,\n",
    "            dist_coeffs,\n",
    "        )\n",
    "        vertices_2d = vertices_2d.swapaxes(0, 1)\n",
    "    else:\n",
    "        vertices_2d, _ = cv2.projectPoints(\n",
    "            mesh.vertices,\n",
    "            np.zeros(3),\n",
    "            np.zeros(3),\n",
    "            camera_matrix,\n",
    "            dist_coeffs\n",
    "        )\n",
    "\n",
    "    img = Image.open(os.path.join(OMNI_IMAGES, img_path))\n",
    "    img = render_mesh(img, img_path, mesh, vertices_2d, output_dir=None, show_on_screen=False)\n",
    "    display(img)\n",
    "    \n",
    "\n",
    "slider = widgets.IntSlider(\n",
    "    value=0,\n",
    "    min=0,\n",
    "    max=len(images)-1,\n",
    "    step=1,\n",
    "    description='Image Index:',\n",
    "    continuous_update=False\n",
    ")\n",
    "\n",
    "widgets.interact(view_image, index=slider)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_filepaths_modified(root, n):\n",
    "    # A function both the humor and wham widgets depend on\n",
    "    cam1_images_path = f\"{root}/omni\"\n",
    "    capture_dir = f\"{root}/capture{n}/rgb\"\n",
    "    sync_file = f\"{root}/synced_filenames_full.txt\"\n",
    "    results_folder = f\"{root}/capture{n}/out_capture{n}/results_out/final_results\"\n",
    "    for path in [cam1_images_path, capture_dir, sync_file, results_folder]:\n",
    "        if not os.path.exists(path):\n",
    "            raise FileNotFoundError(f\"Path {path} does not exist!\")\n",
    "    \n",
    "    return capture_dir, cam1_images_path, sync_file, results_folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Script that uses the saved non-reprojected HuMoR meshes, reprojects them and then renders them on the omni images.\n",
    "from reproject.reproject_mesh.reprojection_utils import get_calib_paths\n",
    "\n",
    "# Define paths\n",
    "n = 1\n",
    "ROOT_DIR = \"/home/NAS-mountpoint/kinect-omni-ego/2024-01-12/at-unis/lab/sid\"\n",
    "HUMOR_MESH_FILE = os.path.join(ROOT_DIR, f\"capture{n}\", f\"out_capture{n}\", \"results_out\", \"final_results\", \"stage3_results.npz\")\n",
    "OMNI_CALIB_FILE = \"/home/sid/Projects/reprojection/calibration/intrinsics/omni_calib.pkl\"\n",
    "\n",
    "CAM0_IMAGES_DIR, CAM1_IMAGES_DIR, SYNC_FILE, results_folder = get_filepaths_modified(ROOT_DIR, n)\n",
    "KINECT_JSONPATH, OMNI_JSONPATH, CAM0_TO_WORLD_PTH, WORLD_TO_CAM1_PTH = get_calib_paths(ROOT_DIR, use_matlab=False, n=n)\n",
    "\n",
    "# Load camera calibration\n",
    "with open(OMNI_CALIB_FILE, \"rb\") as f:\n",
    "    camera_calib = pickle.load(f)\n",
    "use_omni, camera_matrix, xi, dist_coeffs = get_camera_params(camera_calib)\n",
    "\n",
    "# Load meshes from HUMOR_MESH_FILE\n",
    "print(\"[*] Loading meshes from HUMOR_MESH_FILE...\")\n",
    "pred_body = get_meshes(os.path.dirname(HUMOR_MESH_FILE))\n",
    "\n",
    "# Transform meshes\n",
    "print(\"[*] Transforming meshes...\")\n",
    "transformed_meshes = transform_meshes(\n",
    "    pred_body,\n",
    "    CAM0_TO_WORLD_PTH,\n",
    "    WORLD_TO_CAM1_PTH,\n",
    "    KINECT_JSONPATH,\n",
    "    OMNI_JSONPATH,\n",
    "    use_matlab=False\n",
    ")\n",
    "\n",
    "# Synchronize meshes with images\n",
    "print(\"[*] Synchronizing meshes with images...\")\n",
    "synced_cam1_files, synced_meshes = get_synced_meshes(\n",
    "    SYNC_FILE,\n",
    "    CAM0_IMAGES_DIR,\n",
    "    transformed_meshes,\n",
    "    n=n\n",
    ")\n",
    "\n",
    "# Filter images\n",
    "images = [os.path.join(CAM1_IMAGES_DIR, img) for img in sorted(os.listdir(CAM1_IMAGES_DIR)) if img in synced_cam1_files]\n",
    "mesh_files = synced_meshes\n",
    "assert len(images) == len(mesh_files), \"Number of images and meshes do not match\"\n",
    "\n",
    "\n",
    "# Define view_image function\n",
    "def view_image(index, use_omni=True, camera_matrix=camera_matrix, xi=xi, dist_coeffs=dist_coeffs):\n",
    "    mesh = mesh_files[index]\n",
    "    img_path = images[index]\n",
    "    \n",
    "    if use_omni:\n",
    "        vertices_2d, _ = cv2.omnidir.projectPoints(\n",
    "            np.expand_dims(mesh.vertices, axis=0),\n",
    "            np.zeros(3),\n",
    "            np.zeros(3),\n",
    "            camera_matrix,\n",
    "            xi,\n",
    "            dist_coeffs,\n",
    "        )\n",
    "        vertices_2d = vertices_2d.swapaxes(0, 1)\n",
    "    else:\n",
    "        vertices_2d, _ = cv2.projectPoints(\n",
    "            mesh.vertices,\n",
    "            np.zeros(3),\n",
    "            np.zeros(3),\n",
    "            camera_matrix,\n",
    "            dist_coeffs\n",
    "        )\n",
    "    \n",
    "    img = Image.open(img_path)\n",
    "    img = render_mesh(\n",
    "        img,\n",
    "        img_path,\n",
    "        mesh,\n",
    "        vertices_2d,\n",
    "        output_dir=None,\n",
    "        show_on_screen=False\n",
    "    )\n",
    "    display(img)\n",
    "\n",
    "# Create slider widget\n",
    "slider = widgets.IntSlider(\n",
    "    value=0,\n",
    "    min=0,\n",
    "    max=len(images)-1,\n",
    "    step=1,\n",
    "    description='Image Index:',\n",
    "    continuous_update=False\n",
    ")\n",
    "\n",
    "# Link the slider to the view_image function\n",
    "widgets.interact(view_image, index=slider, use_omni=True, camera_matrix=widgets.fixed(camera_matrix), xi=widgets.fixed(xi), dist_coeffs=widgets.fixed(dist_coeffs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-18 20:19:15,108 - INFO - Loaded sync data with 2576 entries\n",
      "2024-12-18 20:19:15,109 - INFO - Initialized renderer with camera calibration, using omni: True\n",
      "2024-12-18 20:19:15,196 - INFO - Sanitizing WHAM output\n",
      "100%|ââââââââââ| 11/11 [00:00<00:00, 11258.50it/s]\n",
      "2024-12-18 20:19:15,199 - INFO - Collating WHAM meshes for 11 sequences\n",
      "100%|ââââââââââ| 11/11 [00:01<00:00,  7.30it/s]\n",
      "2024-12-18 20:19:16,711 - INFO - Transforming 3866 meshes\n",
      "100%|ââââââââââ| 3866/3866 [00:02<00:00, 1300.29it/s]\n",
      "2024-12-18 20:19:20,094 - INFO - Synchronized 2196 meshes with camera files\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9fad863fb93e425289109d0d80f8ec91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=0, continuous_update=False, description='Frame:', max=2195), Output()), â¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.view_image_notebook(idx, base_dir, synced_files, meshes=None, renderer=None)>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Script that renders the non-reprojected WHAM meshes on omni, based on the HuMoR script above.\n",
    "n = 0\n",
    "ROOT_DIR = \"/home/NAS-mountpoint/kinect-omni-ego/2024-01-12/at-unis/lab/sid\"\n",
    "WHAM_MESH_FILE = f'/home/sid/Projects/WHAM/output/demo/2024-01-12_at-unis_lab_sid_capture0/rgb/wham_output.pkl'\n",
    "BM_PATH = \"/home/sid/Projects/humor/body_models/smplh/male/model.npz\"\n",
    "OMNI_CALIB_FILE = \"/home/sid/Projects/reprojection/calibration/intrinsics/omni_calib.pkl\"\n",
    "\n",
    "with open(OMNI_CALIB_FILE, \"rb\") as f:\n",
    "    camera_calib = pickle.load(f)\n",
    "\n",
    "use_omni, camera_matrix, xi, dist_coeffs = get_camera_params(camera_calib)\n",
    "CAM0_IMAGES_DIR, CAM1_IMAGES_DIR, SYNC_FILE, _ = get_filepaths_modified(ROOT_DIR, n)\n",
    "KINECT_JSONPATH, OMNI_JSONPATH, CAM0_TO_WORLD_PTH, WORLD_TO_CAM1_PTH = get_calib_paths(ROOT_DIR, use_matlab=False, n=n)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "with open(WHAM_MESH_FILE, \"rb\") as file:\n",
    "    wham_output_0 = joblib.load(file)\n",
    "\n",
    "with open(CAM0_TO_WORLD_PTH, \"rb\") as f:\n",
    "    cam0_to_world = make_44(pickle.load(f))\n",
    "with open(WORLD_TO_CAM1_PTH, \"rb\") as f:\n",
    "    world_to_cam1 = make_44(pickle.load(f))\n",
    "\n",
    "transform = world_to_cam1 @ cam0_to_world\n",
    "transform[:3, 3] = transform[:3, 3] / 1000.0\n",
    "\n",
    "mesh_sync = MeshSynchronizer(SYNC_FILE, CAM0_IMAGES_DIR)\n",
    "mesh_processor = MeshProcessor()\n",
    "renderer = Renderer(pickle.load(open(OMNI_CALIB_FILE, \"rb\")))\n",
    "pred_bodies = mesh_processor.load_meshes(os.path.dirname(WHAM_MESH_FILE))\n",
    "transformed_meshes = mesh_processor.transform_meshes(pred_bodies, transform)\n",
    "synced_files, synced_meshes = mesh_sync.get_synced_meshes(transformed_meshes, n)\n",
    "\n",
    "def view_image_notebook(idx, base_dir, synced_files, meshes=None, renderer=None):\n",
    "    \"\"\"Interactive notebook viewer for images and meshes\"\"\"\n",
    "    # Setup matplotlib\n",
    "    plt.clf()\n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "    img_path = os.path.join(base_dir, synced_files[idx])\n",
    "    img = Image.open(img_path)\n",
    "    img = ImageOps.mirror(img)\n",
    "    \n",
    "    # Render meshes if provided\n",
    "    if meshes:\n",
    "        curr_meshes = meshes[idx]\n",
    "        for mesh in curr_meshes:\n",
    "            vertices_2d = renderer.project_vertices(mesh)\n",
    "            img = renderer.render_mesh(img, mesh, vertices_2d)\n",
    "    \n",
    "    # Display in notebook\n",
    "    ax.imshow(img)\n",
    "    ax.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "# Create slider widget\n",
    "slider = widgets.IntSlider(\n",
    "    min=0,\n",
    "    max=len(synced_files)-1,\n",
    "    step=1,\n",
    "    description='Frame:',\n",
    "    continuous_update=False\n",
    ")\n",
    "\n",
    "# Setup interactive display\n",
    "widgets.interact(\n",
    "    view_image_notebook,\n",
    "    idx=slider,\n",
    "    base_dir=widgets.fixed(CAM1_IMAGES_DIR),\n",
    "    synced_files=widgets.fixed(synced_files),\n",
    "    meshes=widgets.fixed(synced_meshes),\n",
    "    renderer=widgets.fixed(renderer)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'intrinsics': array([[713.45902492,   0.        , 617.04302192],\n",
       "        [  0.        , 714.28289621, 360.85253964],\n",
       "        [  0.        ,   0.        ,   1.        ]]),\n",
       " 'distortion': array([[ 0.08663132, -0.14762869, -0.00344924,  0.00165397,  0.06665071]]),\n",
       " 'xi': None,\n",
       " 'img_shape': (720, 1280)}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "K0_CALIB_FILE = \"/home/sid/Projects/reprojection/calibration/intrinsics/k0_rgb_calib.pkl\"\n",
    "with open(K0_CALIB_FILE, \"rb\") as f:\n",
    "    k0_calib = pickle.load(f)\n",
    "k0_calib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fx = k0_calib[\"intrinsics\"][0, 0]\n",
    "fy = k0_calib[\"intrinsics\"][1, 1]\n",
    "cx = k0_calib[\"intrinsics\"][0, 2]\n",
    "cy = k0_calib[\"intrinsics\"][1, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fx, fy, cx, cy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save as txt file with fx fy cx cy\n",
    "np.savetxt(\"k0_intrinsics.txt\", np.array([fx, fy, cx, cy]), fmt=\"%.6f\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "odin",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
